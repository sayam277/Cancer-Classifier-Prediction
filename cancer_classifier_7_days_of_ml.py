# -*- coding: utf-8 -*-
"""cancer_classifier 7 days of ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19KpP9MZaqImLXp5mEopUnN0bt9a_uNn-
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('cancer_classification.csv')

"""# EDA and Feature Engineering"""

df

df.info()

df.describe()

df.corr()['benign_0__mal_1'].sort_values(ascending=False)

df.corr()['benign_0__mal_1'][:-1].sort_values().plot(kind='bar')

sns.countplot(x=df['benign_0__mal_1'])

"""# Train Test split"""

x = df.drop('benign_0__mal_1', axis=1)

y = df['benign_0__mal_1']

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=101)

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression

lg = LogisticRegression()

lg.fit(x_train, y_train)

pred_lg = lg.predict(x_test)

df2 = pd.DataFrame(x_test.iloc[0])
#lg.predict())
#x_test.iloc[0]
#np.array(x_test.iloc[2])

np.array(pd.DataFrame(x_test.iloc[4])).reshape(1,30)

lg.predict(np.array(pd.DataFrame(x_test.iloc[89])).reshape(1,30))

y_test.iloc[89]

x_te
#lg.predict(df2)

from sklearn.metrics import confusion_matrix, classification_report

from sklearn.metrics import accuracy_score
print(confusion_matrix(y_test, pred_lg))
print('\n')
print(classification_report(y_test, pred_lg))
print("Accuracy : "+str(accuracy_score(y_test,pred_lg)))

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=2)

knn.fit(x_train, y_train)

pred_knn = knn.predict(x_test)

print(confusion_matrix(y_test, pred_knn))
print('\n')
print(classification_report(y_test, pred_knn))
print("Accuracy : "+str(accuracy_score(y_test,pred_knn)))

"""Choosing the best k value"""

acc_scores = []
for i in range(1,399):
  knn = KNeighborsClassifier(n_neighbors=i)
  knn.fit(x_train,y_train)
  pred_knn = knn.predict(x_test)
  acc_scores.append(accuracy_score(y_test,pred_knn))

plt.figure(figsize=(10,6))
plt.plot(range(1,399),acc_scores,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Accuracy vs. K Value')
plt.xlabel('K')
plt.ylabel('Accuracy')

acc_scores.index(max(acc_scores))+1

"""Hence we get the best value k=1"""

#reconstructing our model with k=1
knn  = KNeighborsClassifier(n_neighbors=1)
knn.fit(x_train, y_train)
pred_knn = knn.predict(x_test)

print(confusion_matrix(y_test, pred_knn))
print('\n')
print(classification_report(y_test, pred_knn))

"""# Decision Trees"""

from sklearn.tree import DecisionTreeClassifier

dtrees = DecisionTreeClassifier()

dtrees.fit(x_train, y_train)

pred_dt = dtrees.predict(x_test)

print(confusion_matrix(y_test, pred_dt))
print('\n')
print(classification_report(y_test, pred_dt))

"""# Random Forests"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=400)

rf.fit(x_train, y_train)

pred_rf = rf.predict(x_test)

print(confusion_matrix(y_test, pred_rf))
print('\n')
print(classification_report(y_test, pred_rf))

"""# SVM"""

from sklearn.svm import SVC

svm = SVC()

svm.fit(x_train, y_train)

pred_svm = svm.predict(x_test)

print(confusion_matrix(y_test, pred_svm))
print('\n')
print(classification_report(y_test, pred_svm))

"""We now tend to find the best parameters to improve our svm model"""

from sklearn.model_selection import GridSearchCV

param_grid = {'C':[0.1, 1, 10, 100, 1000], 'gamma':[1, 0.1, 0.01, 0.001, 0.0001]}

grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)

grid.fit(x_train, y_train)

grid.best_params_

pred_svm_grid = grid.predict(x_test)

print(confusion_matrix(y_test, pred_svm_grid))
print('\n')
print(classification_report(y_test, pred_svm_grid))

"""# KMC

This is an unsupervised learning algorithm to organise the given data into a number of clusters. Lets see if we can use this for our prediction.
"""

from sklearn.cluster import KMeans

kmm = KMeans(n_clusters=2)

kmm.fit(x_test)

pred_kmm = kmm.labels_

pred_kmm

print(confusion_matrix(y_test, pred_kmm))
print('\n')
print(classification_report(y_test, pred_kmm))

"""# ANN"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

x_train = scaler.fit_transform(x_train)

x_test = scaler.transform(x_test)

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dense

x_train.shape

model = Sequential()

model.add(Dense(30,activation='relu'))
model.add(Dense(15,activation='relu'))

# BINARY CLASSIFICATION
model.add(Dense(1,activation='sigmoid'))

model.compile(optimizer = 'adam',loss = 'binary_crossentropy')

model.fit(x=x_train,y=y_train,epochs=600,validation_data=(x_test,y_test))

losses = pd.DataFrame(model.history.history)

losses.plot()

predictions=model.predict(x_test)

predictions

final_predictions = []
for x in predictions:
  if x>0.5:
    final_predictions.append([1])
  else:
    final_predictions.append([0])

final_predictions = np.array(final_predictions)

final_predictions

from sklearn.metrics import classification_report,confusion_matrix

print(confusion_matrix(y_test,final_predictions))
print('\n')
print(classification_report(y_test,final_predictions))